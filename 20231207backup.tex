%\documentclass[12pt]{book}
\documentclass[oneside]{book}

\usepackage{esint}  %%%% problematic??
\let\iiint=\relax
\let\idotsint=\relax
\let\iiiint=\relax
\let\iint=\relax
\newcommand{\cint}{\varointctrclockwise}

\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue
}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{geometry}
 \geometry{
 a4paper,
 total={170mm,257mm},
 left=20mm,
 top=20mm,
 }
 %\usepackage{mathrsfs}
%\newfontfamily{\englishfont}{Latin Modern Roman}
\renewcommand{\empty}{\varnothing}
\newcommand{\len}{\mathrm{len}}
\newcommand{\vol}{\mathrm{Vol}}
\newcommand{\sub}{\subseteq}
\newcommand{\tabb}{\phantom{aaaa}}
\newcommand{\inner}[1]{\langle #1\rangle}
\newcommand{\vphi}{\varphi}
\newcommand{\eps}{\varepsilon}
\newcommand{\Z}{\mathbf{Z}}
\newcommand{\F}{\mathbf{F}}
\newcommand{\N}{\mathbf{N}}
\newcommand{\R}{\mathbf{R}}
\renewcommand{\P}{\mathbf{P}}
\newcommand{\E}{\mathbf{E}}
\newcommand{\C}{\mathbf{C}}
\usepackage{mathrsfs}\newcommand{\CC}{\mathscr{C}}\newcommand{\FF}{\mathscr{F}}\newcommand{\LL}{\mathcal{L}}
\newcommand{\T}{\mathbf{T}}
\newcommand{\Q}{\mathbf{Q}}
\newcommand{\GL}{\mathrm{GL}}\newcommand{\SL}{\mathrm{SL}}
\newcommand{\PSL}{\mathrm{PSL}}
\newcommand{\II}{\mathscr{I}}
\newcommand{\ideal}{\trianglelefteq}
\newcommand{\set}[1]{\{ #1\}}
\newcommand{\gen}[1]{\langle #1\rangle}
\newcommand{\fit}[1]{\left( #1\right)}
\newcommand{\inv}{^{-1}}
\newcommand{\abs}[1]{\left\lvert #1\right\rvert}
\newcommand{\norm}[1]{\left\lVert #1\right\rVert}
\renewcommand{\d}{\mathrm{d}}
\newcommand{\dt}{\mathrm{d}t}
\newcommand{\Exp}{\mathrm{Exp}}
\newcommand{\id}{\mathrm{id}}
\newcommand{\lcm}{\mathrm{lcm}}
\newcommand{\var}{\mathrm{Var}}
\renewcommand{\span}{\mathrm{span}}
\newcommand{\spc}{\phantom{-}}
\newcommand{\str}{^*}
\renewcommand{\i}{{\it i}. }
\newcommand{\ii}{{\it ii}. }
\newcommand{\iii}{{\it iii}. }
\newcommand{\iv}{{\it iv}. }
\renewcommand{\v}{{\it v}. }
\newcommand{\vi}{{\it vi}. }
\newcommand{\vii}{{\it vii}. }
\newcommand{\iix}{{\it iix}. }
\newcommand{\ix}{{\it ix}. }
\newcommand{\x}{{\it x}. }
\newcommand{\nin}{\not\in}
\newcommand{\iid}{i.i.d }
\renewcommand{\H}{\mathbf{H}}
\newcommand{\D}{\mathbf{D}}
\newcommand{\RR}{\mathscr{R}}
%\newcommand{\matt}[4]{\begin{pmatrix} #1 & #2 \\ #3 & #4\end{pmatrix}}
\newcommand*\pmat[4]{\begin{pmatrix}#1&#2\\#3&#4\end{pmatrix}}

\def\Xint#1{\mathchoice{\XXint\displaystyle\textstyle{#1}}%
    {\XXint\textstyle\scriptstyle{#1}}%
    {\XXint\scriptstyle\scriptscriptstyle{#1}}%
    {\XXint\scriptscriptstyle\scriptscriptstyle{#1}}%
      \!\int}
\def\dashint{\Xint-}


\newcommand\chap[1]{%
  \chapter*{#1}%
  \addcontentsline{toc}{chapter}{#1}}



\title{A book dedicated to lovers of mathematics}
\author{}
\date{version \the\year/\the\month/\the\day}

\setlength{\parindent}{0pt}
\begin{document}
\maketitle

%   %   %   %   %   %   %   %   %   %   %   %   %   %   %   %
% SECTION = TABLE OF CONTENTS
\pagestyle{empty}
\tableofcontents

\phantom{}\\
Table of contents:  \\
Chapter 0. Problems   \\
Chapter 1. Euclidean Geometry  \\
Chapter 2. Calculus  \\
Chapter 3. Number Theory \\
Chapter 4. Combinatorics and Probability    \\
Chapter 5. Abstract Algebra    \\
Chapter 6. Analysis  \\
Chapter 7. Topology  \\
Chapter -1. Solutions   \\
Chapter -2. Notes   \\ 

\newpage
%   %   %   %   %   %   %   %   %   %   %   %   %   %   %   %
% SECTION = CHAPTER 0 - PROBLEMS
\chap{0. Problems}
id aa. Consider the following stochastic process. First, a probability $p$ is drawn from a uniform distribution on $[0,1]$. Then $100$ independent coins are drawn, each with probability $p$ to show heads. Without integrals, determine the probability of seeing precisely $k$ heads.    \\\\


id ab. A group of $n$ mathematicians will face the following challenge. Each will be given a hat showing an integer from $1,\dots,n$ (repetitions and misses allowed), and will be able to see the numbers written on all hats but their own. Once they see each other, and without any communication between them, they will all have to simultaneously guess the number written on their hat. They will win if at least one of them makes a correct guess. They have today to form a strategy that will guarantee a win. What do you suggest?   \\\\


id ad. Let $f:\Z^\N\to\Z$ be an additive mapping (namely $f(x+y)=f(x)+f(y)$) from infinite integer sequences to integers. Show that $f(x_1,x_2,x_3,\dots)=\alpha_1 x_1+\dots+\alpha_N x_N$ for some finite integer sequence $\alpha_1,\dots,\alpha_N$.  \\\\


id ah. Let $f:[0,1]\to\R$ be a continuous function for which $\displaystyle\lim_{h\to0}\dfrac{f(t_0+h)+f(t_0-h)-2f(t_0)}{h^2}=0$ for all $t_0\in(0,1)$. Then $f(t)=\alpha t+\beta$ is linear.   \\\\


id ai. Smooth (or even just continuous) homomorphisms $\phi:\R\to\GL_d(\C)$ are in $1:1$ correspondence with $A\in\C^{d\times d}$ via $$\phi(t)=\exp(At)$$ $$A=\dfrac{\d \phi(t)}{\d t}\biggr\rvert_{t=0}$$ \\


id aj. Let $a_n$ be a sequence for which $\sum|a_nb_n|<\infty$ for all $b_n\in\ell^2$. Then $a_n\in\ell^2$.     \\\\


id ak. A two player game goes as follows. Each player is given a number, drawn at random, independently, uniformly from $[0,1]$. A player may keep their number, or ask for another - in which case they must keep the second number. Then the two players reveal their numbers, and the player with the bigger number wins. Find the optimal playing strategy.     \\\\


id al. $n$ ants are placed inside a circle of radius $R$, each is initially heading east,west,north, or south, and all ants have constant speed $1$. When two ants facing opposite directions collide, they both immediately turn $90$ degrees. Find the minimal length of time to guarantee all ants will leave the circle.        \\\\


id am. Customers arrive to a store via a Poisson process (with some constant). For every arrival, the employee calculates the probability this will be the last customer of his shift. At the end of their shift, they write down the probability calculated for the last customer. Over many shifts, what is the distribution of probabilities?    \\\\


id an. Two players take turns coloring the vertices of a graph. In the beginning, all vertices are white. Player 1 picks the first vertex and colors it black. Afterwards, the next player must color black a vertex that is currently white, and adjacent to the the last colored vertex. A player unable to do so losses the game. What property of the graph is equivalent to the first/second player having a winning strategy, and what is their strategy?     \\\\


id ao. A very long road contains $n$ cars, each having a constant speed drawn at random from $20$ kmh to $200$ kmh. The road is narrow so there's no overtaking, so fast cars may have to slow down to match the speed of the car in front. Eventually, how many meshes of connected cars are to be expected?   \\\\


id ap. Alice and Bob play the following game. There is an outer circle, fixed in position, containing $n$ lamps, and an inner circle, which may be rotated, containing the numbers $1..n$. Alice wins when all the lights are off, and Bob wins if she gives up/the game goes forever. Bob chooses the initial on/off states of the lamps. Every turn, Alice specifies a set of numbers in $1..n$ and tells Bob to switch the state (on $\leftrightarrow$ off) of the lamps corresponding to those positions, but Bob may spin the inner circle of numbers before doing so. For which values of $n$ can Alice/Bob win, and how?     \\\\


id aq. A rectangular grid $R$ is painted using $10$ colors. A rectangle is called {\it special} if its four vertices have the same color. Find dimensions for $R$ guaranteeing the existence of a special rectangle.    \\\\


id ar. There exists an uncountable family $\FF$ of infinite subsets of $\N$ where the intersection of any pair of sets in $\FF$ is finite.  \\\\

id as. You find yourself in a huge circular train (say about the size of the equator) in which you can walk freely in both directions. You'll be free once you determine the exact number of cars, but you only have one guess. All the cars look exactly the same, and you can't leave any mark, except each car has a lamp with a switch which you may freely use. However, the initial state of the lamps is random. What do you do?   \\\\


id at. An invisible frog lives on the integer number line, and each day it hops by a fixed unknown amount $d$. Every day you get one guess as to where the frog is, and if you hit it you win a prize. Find a strategy that guarantees you'll win the prize, eventually.\\\\
Part ii. Now the frog moved to the real line, and still hops every day by a fixed amount. It also gained an unknown small but positive length $\eps$. What's your strategy? \\\\


id au. A standard $2$-dimensional Gaussian is given in polar coordinates by $\sqrt{2\log(1/u)}(\cos2\pi\theta,\sin2\pi\theta)$ where $u,\theta$ are independent and uniform on $[0,1]$. \\\\

id aw. There's a deck with $100$ numbered $1..100$ in some order. In each step we peek at the top card, say numbered $k$, take the first $k$ cards from the deck, and shuffle them so that $k$ becomes the bottom card, and then return them on top of the rest of the deck. In particular, after this move the $k$-th card now shows $k$. Show that eventually the top card becomes $1$.  \\\\


id ay. Let $f:[0,1]\to\R$ be a continuous function satisfying $\int_0^1 f(x)x^k=1$ for $k=0,\dots,n-1$. What is the best lower bound on $\int_0^1 f^2$? \\\\


id bd. From each vertex of a triangle, extend the two edges by the length of the opposite edge, then the six endpoints lie on a circle with center $I$ and radius $\sqrt{r^2+s^2}$. \\\\


id bq. If $a_n\to a$ then $\fit{1+\frac{a_n}{n}}^n\to e^a$. \\\\


id br. A betting game with odds $p>1/2$ in your favour goes as follows. In each round, you may wager an amount $x$ of your capital. If the odds were right you get back $2x$, and if not you lose the initial $x$. You start with $\$1$ and the game goes for $n$ rounds. What strategy should be played to maximize the expected value of your capital at the end of the game? What about maximizing the expected value of the $\log$ of your capital. \\\\


id bs. Compute the mean and variance of $e^X$ where  $X\sim N(\mu,\sigma^2)$ is normal. \\\\


id bu. Given a sequence of distribution functions $F_n$ there exists a right-continuous weakly-increasing function $F:\R\to[0,1]$ and a sub-sequence $F_{s(n)}$ for which $\displaystyle\lim_{n\to\infty}F_{s(n)}(x)=F(x)$ for all points of continuity $x$ of $F$. Moreover, $F$ is a distribution function if $\forall\eps\spc\exists a<b\spc\forall n\spc F_n(b)-F_n(a)\ge 1-\eps$.\\\\


id cf. $$\dfrac{1^5}{1+e^\pi}+\dfrac{3^5}{1+e^{3\pi}}+\dfrac{5^5}{1+e^{5\pi}}+\dfrac{7^5}{1+e^{7\pi}}\dots = \dfrac{31}{504}$$\\


id cq. $\SL_2(\Z)$ is generated by $T=\pmat{1}{1}{1}{0}$ and $S=\pmat{0}{-1}{1}{0}$.\\\\


id ct. \\
\i If $r_n$ is a sequence with $r_{n}+1=r_{n-1}r_{n+1}$ then it is $5$-periodic.\\
\ii If $r_n$ is a sequence with $\abs{r_n}=r_{n-1}+r_{n+1}$ then it is $9$-periodic.\\\\


id cu. $100$ mathematicians are standing in a row, each has a hat that's either black or white. Each mathematician only sees in front of him. Starting from the one in the back, each guesses the color of their hat, and the rest hear the guesses. How can they guarantee a maximal number of correct guesses?\\\\
What if the row of mathematicians is infinite?\\\\


id cv. Two mathematicians are in a room, each has a hat showing a rational number. They see each other but can't communicate. Each mathematician writes down a finite list of guesses for their own number. What strategy will ensure that one of them guesses correctly?\\\\
How can we replace the rationals with an uncountable set, while replacing finite guesses with countable guesses?\\\\
How can we replace the rationals with an uncountable set, keep the finite guesses but with three mathmaticians?\\\\


id cw. The audience picks a sequence $a=(a_n)\in\R^\N$. There's $100$ mathemagicians, each in his turn will inspect all but one of the elements of $a$ and will make a guess for the element they did not inspect. Without communicating with each other, $99$ of them will make the correct guess. How is the trick done?\\\\  


id cx. Two mathemagicians perform a trick to the audience using $64$ coins on an $8\times 8$ board. The audience places a coin on each square, choosing whether they show heads or tails. They choose one special square, and allow the first magician who is with them in the room to flip the coin on only one square. Then the second magician enters, sees the new board, thinks a bit, and correctly guesses which special square the audience picked.\\\\
What if we replace $64$ with infinity?\\\\


id cy. $M$ is a well ordered set of magicians. Each magician has a Black/White hat, and only sees the magicians bigger than him. Without communicating, each guesses the color of their hat. Only finitely many get it wrong. How is the trick done?\\\\


id cz. Three mathematicians are in a prison, each in their own cell with a light bulb controlled by the warden. The warden secretely flips a coin and chooses some $N$. For the first $N$ days the lights in the rooms are chosen by the warden at random. For the remaining (infinite number of) days, if the coin turned heads: one random prisoner each day will see light and the rest see dark, and if the coin turned tails: one random prisoner each day will see dark and the rest see light. Afterwards the warden will ask each prisoner if the coin showed heads or tails. Two guess correctly. How?\\\\


id da. Construct a function $f:(0,\infty)\to [0,\infty)$ with $x<f(x)$ and $\abs{\set{\lim_n f^{\circ n}(x)}: x>0}=2^{\aleph_0}$.\\\\


id db. Construct a function $f:\R\to\N$ such that $f(x)=f(x+r)=f(x+2r)\implies r=0$.\\\\


id dc. $31$ prisoners are in a room, the warden randomly drew black and white hats for each of them. They can see each other, but can't see their own hat or communicate. The bell rings and each of them has to either: guess their color is white, guess their color is black, or pass. If any one of them guesses the wrong color or they all pass, they fail. Otherwise they win. Find a strategy that ensures they'll win at least $95\%$ of the time.\\\\


id dd. Similar to [cu] with the three colors $R,G,B$. Only problem is the mathematicians are in six camps as to how to map the colors to $0,1,2$ and can't agree. Find a color-symmetric protocol to guarantee all but one guess correctly.\\\\


id de. Let $f:\T\to\R^d$ be a smooth $2\pi$ periodic function with zero mean $\int_\T f=0$. Then $\norm{f}_\infty\le \frac{1}{2}\norm{f'}_{1}$.\\\\


id df. {\it An awful sorting algorithm} A shelf contains $n$ tombs of the encyclopedia in unsorted order. Every time the librarian notices a tomb not in its right place, he removes it, shifts all the tombs in between, and inserts it in. For example, $(2,4,3,5,1)\to(2,3,5,4,1)\to(2,3,4,1,5)\to(1,2,3,4,5)$ are valid moves. Show that no matter what tombs are picked, after at most $2^n$ steps the shelf will be sorted.\\\\



\newpage
%   %   %   %   %   %   %   %   %   %   %   %   %   %   %   %
% SECTION = CHAPTER 2 - CALCULUS
\chap{2. Calculus}
This chapter emphasizes intuition over rigour. Proofs will later be given in the Analysis chapter.   \\\\


id ac. Fact: For all $x\in\R$ we have $$\exp(x)=e^x=\sum_{n=0}^\infty \frac{x^n}{n!} = \lim_{n\to\infty}\fit{1+\frac{x}{n}}^n$$     \\\\


Nomenclature. The {\it Laplace transform} of a bounded continuous function $f:[0,\infty)\to\R$ is $$(\LL[f])(\lambda)=\int_0^\infty e^{-\lambda x}f(x)\d x$$
defined for $\lambda > 0$. \\\\


id bi. Fact: We have $$\LL[f'](\lambda)=\lambda\LL[f] - f(0)$$ \\


id ca. Fact: The De-Rham cohomology of $\R^2\setminus\set{(0,0)}$ is $H^0\simeq \R$, $H^1\simeq \R$, $H^2=0$. \\\\


id ca. Explanation: The punctured plane is connected, so $H^0=\set{\text{constant functions}}$. A closed $1$-form $\omega = u(x,y)\d x+v(x,y)\d y$ has $u_y=v_x$, and so $ \cint_{\partial R}\omega = 0$ for any rectangle $R$ not including the origin. It is easy to see that there is a common value $\nu(\omega)$ of $\cint_{\partial R}\omega$ for all rectangles $R$ around the origin. We'll show that $\omega$ is exact iff $\nu(\omega)= 0$. Indeed, if $\omega = \d f$ then $\oint \omega=0$ for any loop. On the other hand, if $\nu(\omega)=0$ then there is a well defined smooth function $f(x,y)=\int_{\gamma} \omega$, letting $\gamma$ be any curve piece-wise parallel to the axes, going from $(1,0)$ to $(x,y)$ not through the origin, and we have $\d f=\omega$. A closed $1$-form which is not exact is $\alpha="\d \theta"=\dfrac{x\d y-y\d x}{x^2+y^2}$. Indeed we have $\d \alpha = 0$ and $\cint_{x^2+y^2=1}\alpha = 2\pi$. It follows that any closed $1$-form is uniquely a constant times $\alpha$ plus an exact form, or $H^1=\R\alpha$. Finally we must show any $2$-form $\eta=f(x,y)\d x\wedge \d y$ is exact. An engineer's solution would be to write
$\eta = \tilde{f}(r,\theta)r\d r\wedge \d\theta = \d\fit{\fit{\int_1^r \tilde{f}(t,\theta)t\d t}\d \theta}$. One can bring this back rigorously to $x,y$ coordinates and verify it works.  \\\\


id cp. Fact: For all $z\in\C$ we have $$\pi\cot(\pi z)=\dfrac{1}{z}+\sum_{m\ge 1}\dfrac{1}{z+m}+\dfrac{1}{z-m}$$\\


id cp. Explanation: Apply a logarithmic derivative to the sine product. \\\\


id cp. Explanation 2: It is easy to see the series converges to a meromorphic function of period $1$ with simple poles at the integers with residues $1$. Hence the difference between the two sides is an entire function. But one can see that the difference is bounded, hence constant. Developing to first order at $z=0$ we find the two sides are equal.\\\\


\subsection*{Elliptic functions}


Nomenclature. The Weierstrass function of a $2$ dimensional lattice $L$ in $\C$ is $\wp(z)=z^{-2}+\displaystyle\sum_{\omega\in L\setminus 0}\fit{(z-\omega)^{-2}-\omega^{-2}}$ \\\\


id cb. Fact: \\
\i $\wp$ is a well defined $L$-periodic even meromorphic function of order $2$ with double poles on the points of $L$. We have $\wp'(\eta)=0$ for $\eta\in \frac{1}{2}L \setminus L$.\\
\ii The expansion of $\wp$ at the origin is $\wp(z)=z^{-2}+\sum_{n\ge1}(2n+1)G_{2n+2}z^{2n}$, where $G_{m}=\sum_{\omega\in L\setminus0}\omega^{-m}$. \\
\iii We have $$\wp'(z)^2=4\wp(z)^3-g_2\wp(z)-g_3=4(\wp(z)-e_1)(\wp(z)-e_2)(\wp(z)-e_3)$$ where $g_2=60G_4, g_3=140G_6,  e_1=\wp(\frac{\omega_1}{2}), e_2=\wp(\frac{\omega_2}{2}), e_3=\wp(\frac{\omega_1+\omega_2}{2})$ for $L=\Z \omega_1\oplus \Z\omega_2$ \\
\iv We have $$\begin{vmatrix}
1 & \wp(u) & \wp'(u) \\ 
1 & \wp(z) & \wp'(z) \\ 
1 & \wp(z+u) & -\wp'(z+u)
\end{vmatrix}=0$$ \\
\v $$p(u)+p(z)+p(u+z)=\dfrac{1}{4}\fit{\dfrac{\wp'(u)-\wp'(z)}{\wp(u)-\wp(z)}}^2$$\\


id cb. Explanation: It's simple to verify $\wp$ is a well defined even meromorphic function with double poles on the points of $L$. Clearly $\wp'(z)=-2\displaystyle\sum_{\omega\in L}(z-\omega)^{-3}$ is $L$-periodic. Hence for $\omega\in L$, $\wp(z+\omega)-\wp(z)$ is a constant. But $\wp$ is even, so this constant is zero and $\wp$ is $L$-periodic of order $2$. For $\eta$ a proper half lattice point, we see that $\wp'(\eta)=0$ either from the series for $\wp'$, or by deriving the identity $\wp(z)-\wp(2\eta-z)=0$ at $\eta$. The expansion at $0$ is clear, and the left=center identity follows because the difference is $L$-periodic without poles with no zero-th order terms. Since $\wp$ has order $2$, the values $e_1,e_2,e_3$ are distinct (each is assumed with multiplicity $2$). Thus $e_1,e_2,e_3$ are precisely the three roots of the polynomial $4t^2-60G_4t-140G_6$. Now, given (general) $z_1,z_2$ we find $a,b$ which solve $b=a\wp(z_i)+\wp'(z_i)$. Then $f(z)=\wp'(z)-a\wp(z)$ is an elliptic function of $L$ of order $3$ with a triple pole at $0$ and roots at $z_1,z_2$. Since the sum of the three roots minus poles of $f$ (inside a parallelogram) is congruent to $0$ mod $L$, $-z_1-z_2$ is a root of $f$, yielding the determinant formula. Finally, the line $y=ax+b$ and cubic $y^2=4x^3-g_2x-g_3$ intersect at $(\wp(z_i),\wp'(z_i))$ and $(\wp(-z_1-z_2),\wp'(-z_1-z_2))$. It follows that $4x^3-a^2x^2-(g_2+2ab)x-(g_2+b^2)$ has roots at $x=\wp(z_i),\wp(-z_1-z_2)$ and so the sum of the roots is $a^2/4$, yielding \v\\\\

id cc. Fact: For $\tau\in\H, q=e^{2\pi i\tau}\in\D^\times$ we have
$$\sum_{m\in\Z}(m+\tau)^{-4}=\dfrac{8\pi^4}{3}\sum_{\ell=1}^\infty \ell^3 q^{\ell}$$
$$\sum_{m\in\Z}(m+\tau)^{-6}=-\dfrac{8\pi^6}{15}\sum_{\ell=1}^\infty \ell^5 q^{\ell}$$
$$g_2(\tau)=\dfrac{4\pi^4}{3}\fit{1+240\sum_{\ell=1}^\infty \sigma_3(\ell) q^\ell}$$
$$g_3(\tau)=\dfrac{8\pi^6}{27}\fit{1-504\sum_{\ell=1}^\infty \sigma_5(\ell) q^\ell}$$\\


id cc. Explanation: Recall [??]
$$\dfrac{1}{\tau}+\sum_{m\neq 0}\fit{\dfrac{1}{\tau+m}-\dfrac{1}{m}}=\pi\cot(\pi \tau)=i\pi\dfrac{q+1}{q-1}=-i\pi\fit{1+2\sum_{\ell=1}^\infty q^\ell}$$
repeated differentiation yields the first two identities. Once those are in place, we have $G_{2j}(\tau)=\sum_{n,m}(m+n\tau)^{-2j}=2\zeta(2j)+2\sum_{n,\ell=1}^{\infty}c\ell^{2j-1}q^{n\ell}=2\zeta(2j)+2c\sum_{k=1}^{\infty} \sigma_{2j-1}(k)q^k$\\\\


Nomenclature. $\Delta=\Delta(4t^2-g_2t-g_3)=g_2^3-27g_3^2$ and $J=\dfrac{g_2^3}{g_2^3-27g_3^2}$. These are functions of the lattice $L$, or functions of two generators $\omega_1,\omega_2$\ of $L$, or functions of one $\tau\in\H$ with $L=\Z+\tau\Z$.\\\\


id cr. We have $$\Delta(\tau+1)=\Delta(\tau)=\tau^{-12}\Delta(-1/\tau)$$
$$J(\tau+1)=J(\tau)=J(-1/\tau)$$
And more generally $$\Delta(A\tau)=(c\tau+d)^{12}\Delta(\tau)\spc\spc J(A\tau)=J(\tau)$$
for any $A=\pmat{a}{b}{c}{d}\in\SL_2(\Z)$.\\\\


id ce. Fact: For $\tau\in\H, q=e^{2\pi i\tau}\in\D^\times$ we have $$\Delta(\tau)=(2\pi)^{12}\sum_{n=1}^\infty \tau(n)q^n$$
$$12^3J(\tau)=q\inv + 744 + \sum_{n=1}^\infty c(n)q^n$$
where $\tau(n),c(n)\in\Z$.\\\\


id ce. Explanation: Using [cc] we have $$(2\pi)^{-12}\Delta=(2\pi)^{-12}\fit{g_2^3-27g_3^2}=\dfrac{1}{27\cdot 64}\fit{(1+240A)^3 - (1-504B)^2} = \dfrac{5A+7B}{12}+ C $$ 
So we need to show $12\mid 5\sigma_3(n)+7\sigma_5(n)$ or simply $12\mid 5k^3+7k^5$. Now we notice $\tau(1)=1, \tau(2)=-24$ and so
$$12^3J = 12^3 \dfrac{g_2^3}{\Delta} = \dfrac{12^3 \cdot 64\pi^{12}(1+240A)^3}{27(2\pi)^{12}q(1-24q-Eq^2)}=\dfrac{(1+240A)^3}{q(1-24q-Eq^2)}=q\inv(1+3\cdot 240q+\dots)(1+24q+\dots)$$\\


Nomenclature. A {\it modular function} is a meromorphic function on $\H\cup{i\infty}$ invariant under $\PSL_2(\Z)$. Namely, if $g$ is a meromorphic function on $\D$, and $f(\tau)=g(e^{2\pi i\tau})$ satisfies $f(\tau)=f(-1/\tau)$ then $f$ is a {\it modular function}.\\\\


Example. $J$ is a modular function.\\\\


Nomenclature. $\RR=\set{\tau\in\H: \abs{\tau}>1, -0.5\le\Re(\tau)< 0.5}\cup \exp(i[\pi/2,2\pi/3])$ is the {\it fundamental region}.\\\\


id cg. Fact: Each $\tau\in \H$ has exactly one $\tau'\in \RR$ for which $\tau\simeq \tau'$ under $\PSL_2(\Z)$. \\
Moreover, if $A\tau=\tau$ and $A\neq\id$ then $\tau = i,e^{2\pi i/3}$. \\
The solutions in case $\tau=i$ are $Az=z,\dfrac{-1}{z}$ and for $\tau=e^{2\pi i/3}$ are $Az=z, \dfrac{-1}{z+1}, \dfrac{-z-1}{z}$. \\\\


id cg. Explanation. Let $\tau'$ be an element of $\PSL_2(\Z)\tau$ of maximal imaginary part. It exists because $\Im A\tau= \dfrac{\Im \tau}{\abs{c\tau+d}^2}$ and $\set{c\tau+d: \gcd(c,d)=1}$ is discrete and has a shortest element. Under $\tau\mapsto \tau\pm1$ we may assume $-0.5\le \Re\tau'< 0.5$. It must be the case that $\abs{\tau'}\ge1$ for otherwise $-1/\tau'$ would have a larger imaginary part. If $\abs{\tau'}>1$ then $\tau'\in\RR$, and if $\abs{\tau'}=1$ then either $\tau'$ or $-1/\tau'$ is in $\RR$. Now we need to see that no two elements of $\RR$ are equivalent. Let $\tau,A\tau\in\RR$. wlog let $\Im A\tau\ge \Im \tau$, so that $1\ge \abs{c\tau+d}\ge \abs{c}\Im\tau\ge \abs{c}\sqrt{3}/2$ and $c=0,\pm1$. If $c=0$ then $Az=z+b$ and clearly if $\tau,A\tau\in\RR$ then $b=0$, $A=\id$ and $\tau=A\tau$. If $c=\pm1$ we may take $c=1$ and so $1\ge \abs{\tau+d}$. Therefore $\tau$ is at most at distance one from an integer, and this integer must be $d=0,1$. If $d=0$ then $\abs{\tau}=1$ and $Az=a-\dfrac{1}{z}$. It is clear that either $a=0, \tau=A\tau=i$ or $a=-1, \tau=A\tau=e^{2\pi i/3}$. Finally, if $d=1$ then $\tau=e^{2\pi i/3}$ and $\Im A\tau=\sqrt{3}/2$ and so $\tau=A\tau=e^{2\pi i/3}$ . \\\\


id ch. Fact: Let $f$ be a non-constant modular function. Then $f$ has an {\it order} $n\ge 1$ such that each value $c\in\C_\infty$ is obtained by $f$ in $\RR\cup\set{i\infty}$ with $\RR$-multiplicity $n$. Here the $\RR$-multiplicities at $i, e^{2\pi i/3}$ are to be divided by $2,3$. In particular, they are divisible by $2,3$.\\\\


id ch. Explanation: It suffices to show $0,\infty$ are obtained with the same multiplicity. There's a finite number of zeros and poles in $\RR$ because there isn't an essential singularity at $\tau=i\infty, q=0$. Let us assume there are no zeros or poles on $\partial\RR$. To get the number of zeros minus the number of poles, we take a sufficiently large $M$ and integrate $\dfrac{f'(\tau)\d \tau}{2\pi if(\tau)}$ over $\partial(\RR\cap \Im\le M)$. The value over the vertical parts cancel, and so do the two symmetric parts of the unit circle. [Indeed $f'(w)/f(w)\d w = f'(z)/f(z)\d z$ for $w=z+1$ or $w=-1/z$] and we're left with only the (signed) order of the zero/pole at $\tau=i\infty$. In case there's zeros or poles on the path of integration, we change it. Only if the zero/pole is at $\tau=i,e^{2\pi i/3}$ the path is a half circle around $i$ and two $60$ degree sectors around $e^{2\pi i/3}, e^{2\pi i/3}+1$.\\\\



id ci. Fact: The order of $J$ is $1$. We have $j(i\infty)=\infty$, $j(i)=1$, $j(e^{2\pi i/3})=0$ with (regular) multiplicities $1,2,3$.\\\\


id cj. Fact: We have $\overline{J(\tau)}={J(-\bar{\tau})}$. Also, $J$ maps $-\frac{1}{2}+i[\frac{\sqrt{3}}{2},\infty)$, $\exp(i[2\pi/3,\pi/2])$, $i[1,\infty)$ to $(-\infty,0]$, $[0,1]$, $[1,\infty)$.\\\\


id ck. Fact: $J$ acts as a covering map of degree $1$ from $\H\setminus \PSL_2(\Z)\set{i,e^{2\pi i/3}}$ to $\C\setminus\set{0,1}$.\\\\


id cl. Fact: An entire function $f:\C\to\C$ omitting two values must be constant.\\\\


id cl. Explanation: If $f$ omits $a,b$ then $g=\dfrac{f-a}{b-a}$ omits $0,1$. Since $\C$ is simply connected, $g$ may be lifted to a holomorphic $h:\C\to\H$ with $g=J\circ h$. But then $h$ must be constant, as must $g$ and $f$.\\\\


id cm. Fact: For $s>0$ we have $$\sum_{n\in\Z}e^{-\pi s n^2}=\dfrac{1}{\sqrt{s}}\sum_{n\in\Z}e^{-\pi n^2/s}$$\\


id cm. Explanation: We apply Poisson summation formula to $e^{-\pi sx^2}$. \\\\


id cn. Fact: More generally, if $a,b,\tau\in\C$ and $\Im\tau>0$ then 
$$\sum_{n\in\Z}e^{i\pi \tau (n+a)^2}e^{-2\pi i b(n+a)}=\dfrac{1}{\sqrt{-i\tau}}\sum_{n\in\Z}e^{-i\pi (n+b)^2/\tau}e^{2\pi i na}$$\\


id cn. Explanation: We apply Poisson summation to $e^{-\pi s(x+a)^2}e^{-2\pi ib(x+a)}$ (and analytically continue to the complex domain).\\\\


Nomenclature. $\eta(\tau)=e^{2\pi i\tau/24}\displaystyle\prod_{k=1}^\infty (1-e^{2\pi ik\tau})$.\\\\


id co. Fact: $\eta$ is holomorphic and non-zero in $\H$. It satisfies \\
\i $\eta(\tau)=\sum_{n\in\Z}(-1)^n e^{3\pi i \tau(n+\frac{1}{6})^2}$\\
\ii $\eta(-1/\tau)=(-i\tau)^{1/2}\eta(\tau)$\\
\iii $\Delta(\tau)=(2\pi)^{12}\eta(\tau)^{24}$\\\\


id co. Explanation: \i follows from the pentagonal identity. To get \ii, we apply [cn] with $\dfrac{1}{6},-\dfrac{1}{2},3\tau$ we get
$$\sum_{n\in\Z}e^{\pi i 3\tau (n+\frac{1}{6})^2}e^{\pi i (n+\frac{1}{6})}=\dfrac{1}{\sqrt{-3i\tau}}\sum_{n\in\Z}e^{-\frac{\pi i}{3\tau} (n-\frac{1}{2})^2}e^{\frac{\pi i n}{3}}$$
The lhs is $e^{\pi i/6}\eta(\tau)$. We split the sum in the rhs to $n=3k,3k+1,3k+2$. The sum for $n=3k+2$ is a constant times $\sum_k (-1)^k e^{-\frac{\pi i}{3\tau}(3k+3/2)^2}$, where the $k$ summand is minus the $-k-1$ summand, so this sum is zero. Switching $k$ to $-k$ in the first sum, we see that it is $\eta(-1/\tau)$, and the second sum is $e^{\frac{\pi i}{3}}\eta(-1/\tau)$. For \iii we see that $f(\tau)=\dfrac{\Delta(\tau)}{(2\pi)^{12}\eta(\tau)^{24}}$ is analytic in $\H\cup{i\infty}$, invariant under $\PSL_2(\Z)$, has no zeros or poles and equals $1$ at $i\infty$. Such a modular function must be constant $1$.\\\\


id cs. Fact: We have $$p\tau_p\Delta(\tau) = p^{12}\Delta(p\tau) + \sum_{k=0}^{p-1} \Delta\fit{\dfrac{\tau + k}{p}}$$\\


id cs. Explanation: For $k\neq 0$ we apply [??] for $\pmat{k}{b}{p}{k^*}\in\SL_2(\Z)$ to get $\Delta\fit{\dfrac{k\tau -1}{p\tau}}=\tau^{12}\Delta\fit{\dfrac{\tau-k^*}{p}}$. If $R(\tau)$ is the rhs, this shows $R(-1/\tau)=\tau^{12}R(\tau)$, but clearly $R$ is analytic on $\H$ and is $1$ periodic. Moreover, we have a Taylor expansion at $\tau\to i\infty$ in terms of $q=e^{2\pi i\tau}$ as $(2\pi)^{-12}R(\tau)=p^{12}\sum_{n=1}^{\infty} \tau_n q^{pn}+p\sum_{n=1}^\infty \tau_{np}q^n $ is analytic. It follows by [??] that $R(\tau)=p\tau_p\Delta(\tau)$ by equating the $q$ coefficient. In particular, we have $$\tau_p\tau_k = \begin{cases}
    \tau_{pk} & p\nmid k\\
    \tau_{pk}+p^{11}\tau_{k/p} & p\mid k
\end{cases}$$






\newpage
%   %   %   %   %   %   %   %   %   %   %   %   %   %   %   %
% SECTION = CHAPTER 3 - NUMBER THEORY
\chap{3. Number Theory}
id ax. Fact: We have $\lcm(1,\dots,n)\ge 2^{n-2}$ for all $n$.    \\\\


id ax. Explanation: Consider $I=\int_0^1 t^m(1-t)^m\d t$. We have $I=\dfrac{r}{\lcm(1,\dots,2m+1)}$ for some positive integer $r$, as well as $I\le 4^{-m}$. Thus $4^{m}\le \lcm(1,\dots,2m+1)$.



\newpage
%   %   %   %   %   %   %   %   %   %   %   %   %   %   %   %
% SECTION = CHAPTER 4 - COMBINATORICS AND PROBABILITY
\chap{4. Combinatorics and Probability}
id az. Fact: A monkey is typing letters uniformly at random. The the expected number of letters until ABRACADABRA is typed equals $26+26^4+26^{11}$.    \\\\


id az. Explanation. Imagine a casino where gamblers can bet on the next letter. There are $26$ letters and the casino gives fair odds, meaning a gambler can pay the casino $\$x$ dollars and bet that the next letter will be $y$. If the bet was right he will get back $\$ 26x$. Now imagine that before every letter, a new gambler arrives with $\$1$ and bets all their money that the next $11$ letters will be ABRACADABRA (of course this will take several turns, the first putting down $\$1$ on the first letter being A. If the bet works out, the next bet would be $\$26$ dollars on the next letter being B, and so on). Starting with $\$0$, let $X_n$ be the cash of the casino after the $n$ letter was typed. Let $T$ be the stopping time - when ABRACADABRA is first typed. Then $X_n$ is a martingale with bounded differences, $\E T<\infty$ and so $\E X_T= \E X_0 = \$0$. Let's consider the cash after time $T$. Every unsuccessful gambler lost $\$1$ to the casino. There are $3$ successful gamblers, who put in an initial $\$1$ and took $26+26^4+26^{11}$ from the casino. In total, $X_{T}=T-26+26^4+26^{11}$.    \\\\


Nomenclature. In a {\it branching process} of microbes, the $0$-th generation consists of a single microbe, and each microbe gives birth to a number of children, randomly generated from a fixed distribution $X$ (taking non-negative integer values) independently of all other microbes. In formula, $Z_0=1$, and $Z_{n+1}=A_{1,n}+\dots+A_{Z_n,n}$, where $A_{i,j}\sim X$ are independent.    \\\\


id bb. Fact: Let $(X,Z_n)_{n=0}^\infty$ be a branching process, and set $f(\theta)=\E[\theta^X]$, $g_n(\theta)=\E[\theta^{Z_n}]$ for $\theta\in[0,1]$. Then \\
\i $g_n=f\circ f\dots \circ f$ is the $n$-th composition of $f$.\\
\ii The {\it probability of extinction} $p=\P[Z_m=0 \spc \text{for some }m]$ is the smallest non-negative fixed point of $f$.   \\\\


id bg. Fact: If the edges of $K_n$ are exactly covered by the edges of $m$ complete bi-partite graphs, then $m\ge n-1$.    \\\\


id bg. Explanation: Suppose $E(K_n)=\bigsqcup_{k=1}^m E(L_k,R_k)$ with $m+1<n$. Fix a later chosen $N$, and for each mapping $\sigma:[n]\to [N]$ let $P(\sigma)=(\sum_{L_1}\sigma,\dots,\sum_{L_m}\sigma,\sum_{[n]}\sigma)$. Then $P$ has domain of size $N^n$ and range of size at most $(nN)^{m+1}$. We choose $N$ guaranteeing $P$ is not injective. Fix $\sigma_1\neq\sigma_2$ with $P(\sigma_1)=P(\sigma_2)$, and set $\tau=\sigma_1-\sigma_2$. We have $\sum_{i<j}\tau(i)\tau(j)=\sum_{k=1}^m \sum_{L_k}\tau\sum_{R_k}\tau=0$, and $\sum\tau(i)=0$. Thus $\sum\tau(i)^2=[\sum\tau(i)]^2-2\sum_{i<j}\tau(i)\tau(j)=0$, in contradiction to $\sigma_1\neq\sigma_2$. \\\\


id bh. Fact: Let $f$ be a bounded continuous function on $[0,\infty)$, $\LL[f]$ its Laplace transform, let $X_1,\dots,X_n$ be a sequence of iid variables $\sim\Exp(\lambda)$ and $S_n=X_1+\dots+X_n$. We have \\
\i $\E[f(S_n)]=\dfrac{(-1)^{n-1}\lambda^n}{(n-1)!}\dfrac{\d^{n-1} \LL[f](\lambda)}{\d \lambda^{n-1}}$.\\
\ii For all $y>0$ we have $f(y)=\displaystyle\lim_{n\to\infty} \fit{\dfrac{(-1)^{n-1}\lambda^n}{(n-1)!}\dfrac{\d^{n-1} \LL[f](\lambda)}{\d \lambda^{n-1}}}\biggr\rvert_{\lambda=n/y}$, a formula for inverting $\LL[f]$ back to $f$.    \\\\


id bn. Fact: Let $S_n$ be a symmetric random walk on $\Z$, $T=\inf\set{n:S_n=1}$. Then $P(T=2m-1)=\displaystyle\binom{\frac{1}{2}}{m}(-1)^{m+1}$. \\\\


id bn. Explanation: We know that $T<\infty$ almost surely (though $\E[T]=\infty$). Let $f(\alpha)=\E[\alpha^T]$. Then $f(\alpha)=\frac{1}{2}\alpha+\frac{1}{2}\alpha \E[\alpha^{T'}]$ where $T'$ is the time to get to $1$, starting at $-1$. Now $T'$ is a sum of two independent variables, both distributed as $T$. Thus $\E[\alpha^{T'}]=f(\alpha)^2$. It follows that $f(\alpha)=\dfrac{1-\sqrt{1-\alpha^2}}{\alpha}$. It remains to compare coefficients. \\\\


id bo. Fact: Let $Z_n$ be a Markov chain on $(E,P)$. Then the following are equivalent.\\
\i For any $i,j\in E$ we have $\P\fit{\exists n\ge 1: j=Z_n\mid Z_0=i}=1$. \\
\ii All $P$-super-harmonic non-negative functions $h$ on $E$ are constant.  \\\\


id bo. Explanation: \i$\implies$ \ii Clearly $h(Z_n)$ is a non-negative super-martingale. Starting at $Z_0=i$, let $T$ be the stopping time of reaching $j$, so $T$ is almost-surely finite, and thus $h(j)\E[h(Z_T)]\le h(Z_0)=h(i)$.\\
\ii$\implies$\i For fixed $j$ the function $h_j(i)=\P[\text{hitting}\spc j\mid Z_0=i]$ is $P$-super-harmonic. Indeed, $\sum_k h_j(k)p_{ik}\le \sum_{k\neq j}h_j(k)p_{ik}+p_{ij}=h_j(i)$. Therefore $h_j\equiv c_j$ is constant and $p_{ij}=p_{ij}c_j$. If $p_{ij}=0$ for all $i$ then the function $\delta_j$ would be $P$-super-harmonic and non-constant, and so $c_j=1$. \\\\


id bs. Fact: Let $X$ be a random variable with distribution function $F$ and characteristic function $\vphi(\theta)=\E[e^{i\theta X}]$. Then for all $a<b$ $$\frac{1}{2\pi i}\int_{-T}^T\dfrac{e^{-i\theta a}-e^{-i\theta b}}{\theta}\vphi(\theta)\d \theta \underset{T\to\infty}{\longrightarrow} \P[x\in(a,b)] + \frac{1}{2}\P[X\in\set{a,b}]$$
In particular, the characteristic function of $X$ determines (the distribution of) $X$. Specifically, if $\int_\R|\vphi| < \infty$ then $X$ has a density function $f$, and we have
$$f(x)=\dfrac{1}{2\pi}\int_\R e^{-i\theta x}\vphi(\theta)\d \theta\tabb\tabb\vphi(\theta)=\int_\R e^{i\theta x}f(x)\d x$$
\\


id bv. Fact: Let $(X_n)$ be a sequence of \iid random variables with mean $\mu$ and variance $\sigma^2$. Then for all $a\in\R$ we have
$$\lim_{n\to\infty}\P\fit{\frac{1}{n}(X_1+\dots+X_n)-\mu \le a\frac{\sigma}{\sqrt{n}}}=\dfrac{1}{\sqrt{2\pi}}\int_{-\infty}^a e^{-\frac{1}{2}x^2}\d x$$
\\


id bv. Explanation: We first show the point-wise convergence of the characteristic functions $\vphi_{G_n}(\theta)$ to the characteristic function of the standard normal. Indeed, $\vphi_{G_n}(\theta)={\vphi\fit{\frac{\theta}{\sqrt{n}\sigma}}}^n$ where $\vphi$ is the characteristic function of $X_i-\mu$. Recall [??] that $\vphi(\alpha)=1-\fit{\frac{1}{2}\sigma^2+o(1)}\alpha^2$ as $\alpha\to 0$. Thus $\vphi_{G_n}(\theta)=\fit{1-{\frac{{\theta^2/2}+o(1)}{n}}}^n$ converges to $e^{-\frac{1}{2}\theta^2}$ as $n$ approaches infinity [bq]. It remains to show the following.   \\\\


Nomenclature. A sequence $(F_n)$ of distribution functions {\it weakly converge} to a distribution function $F$ if $$\lim_{n\to\infty}F_n(x)=F(x)$$
for all points of continuity $x$ of $F$.    \\\\


id bw. Fact: Let $F_n$ be a sequence of distribution functions. Assuming the point-wise convergence of the characteristic functions $\vphi_{F_n}(\theta)$ to a function $g(\theta)$ continuous at $0$. Then $F_n$ weakly converge to a distribution function $F$ and $g(\theta)=\vphi_F(\theta)$ is its characteristic function. 


\newpage
%   %   %   %   %   %   %   %   %   %   %   %   %   %   %   %
% SECTION = CHAPTER 5 - ABSTRACT ALGEBRA
\chap{5. Abstract Algebra}
id bm. Fact: For a subset $S\sub \F_2^d$ let $\Gamma_S(x,y)=\displaystyle\sum_{s\in S}x^{d-W(s)}y^{W(s)}$ where $W(s)=\abs{i\in [d]:s_i=1}$ is the Hamming weight. If $U$ is a subspace of $\F_2^d$ and $W=\set{v\in \F_2^d: u^tv=0 \spc\forall u\in U}$ is the "orthogonal" subspace, then $$\Gamma_W(x,y)=\dfrac{1}{\abs{U}}\Gamma_U(x+y,x-y)$$
In particular, the multiset of Hamming weights of $U$ is sufficient information for that of $W$.    \\\\


id bm. Explanation: Let $f(s)=x^{d-W(s)}y^{W(s)}$ be a function on $\F_2^d$ and $\psi$ be a linear character of $\F_2^d$. We have $\psi=\psi_v:x\mapsto (-1)^{v^tx}$ for some $v\in \F_2^d$. Thus $2^d\hat{f}(\psi)=\sum_{s\in\F_2^d} x^{d-W(s)}y^{W(s)}(-1)^{v^ts}=\prod_{i=1}^d\sum_{s_i=0}^1  x^{1-s_i}y^{s_i}(-1)^{v_is_i}=\prod_{i=1}^d (x+y)^{1-v_i}(x-y)^{v_i}=(x+y)^{d-W(v)}(x-y)^{W(v)}$. It remains to utilize Poisson summation formula. \\\\


id ba. Fact: If $A$ is a Noetherian ring then $A[t]$ is a Noetherian ring.  \\\\


id ba. Explanation: Fix an ideal $\II\ideal A[t]$. A best effort attempt to generate $\II$ is as follows: as long as $f_1(t),...,f_{n-1}(t)$ don't generate $\II$, let $f_n(t)$ be the polynomial of minimal degree in $\II$ and not in $\gen{f_1(t),\dots,f_{n-1}(t)}$. Note that $d_n=\deg f_n(t)$ is weakly increasing, and that the leading coefficient of $f_n(t)$ does not belong to the ideal of $A$ generated by the leading coefficients of  $f_1(t),..,f_{n-1}(t)$, for otherwise there are $a_1,\dots,a_{n-1}\in A$ for which $g(t)=f_n(t)-a_1f_1(t)t^{d_n-d_1}\dots-a_{n-1}f_{n-1}(t)t^{d_n-d_{n-1}}$ has degree smaller than $f_n(t)$ and is also member of $\II\setminus \gen{f_1(t),..,f_n(t)}$, which can't be. It follows that the sequence of ideals of $A$ generated by the leading coefficients of $f_1(t),\dots,f_n(t)$ is strictly increasing, and $A$ is not Noetherian. \\\\


id be. Fact: In a vector space, if $(u_1,\dots,u_i)$ is linearly independent and $(w_1,\dots,w_s)$ is spanning, then $i\le s$. Moreover, we may replace some $i$ members of the latter with the members of the former to produce a spanning set.  \\\\


id be. Explanation: By induction on $i$, the base $i=0$ being trivial. For the step, we have wlog $(u_j)_{j<i}\cup (w_k)_{k\ge i}$ spanning. Writing $u_i$ as a linear combination, a coefficient of one of the $w$'s must be non-zero, wlog say that of $w_i$. Thus $w_i$ is spanned by $(u_j)_{j\le i}\cup (w_k)_{k>i}$. This set is spanning, since it contains the remaining members of the previously spanning set.   \\\\


We also give specific explanations for when $\F$ is a finite field and when $\F=\Q$.\\\\

id be. Explanation for finite fields: If $\abs{\F}=q$ is finite, then the span of $u_1,\dots,u_i$ has precisely $q^i$ elements, where as the span of $w_1,\dots,w_s$ has at most $q^s$ elements.   \\\\


id be. Explanation for $\F=\Q$: Write $u_t=\sum_j\alpha_{tj}w_j$ for $\alpha_{tj}\in\Q$, $t\in[i], j\in[s]$. Let $M$ be a common multiple of the denominators, $f_t=Mu_t$ and $\beta_{tj}=M\alpha_{tj}\in\Z$. Let $R$ be a positive integer variable, $A_R=\{r_1f_1+\dots+r_if_i:r_t\in\Z,\abs{r_t}\le R\}$ and $B_R=\{m_1w_1+\dots+m_sw_s:m_j\in\Z,\abs{m_j}\le CR\}$ where $C=\max_j\sum_t\abs{\beta_{tj}}$. We have $A_R\sub B_R$, and with the polynomial inequality $(2R+1)^i=\abs{A_R}\le\abs{B_R}\le(2CR+1)^s$ valid for all values of $R$ we deduce $i\le s$.   \\\\


id bf. Fact: If $f:\F_2^d\to\pm1$ satisfies $f(x+y)=f(x)f(y)$ in probability $\ge 1-\eps$, then there exists a character $\phi:\F_2^d\to\pm1$ with $f=\phi$ in probability $\ge 1-\eps$. \\\\


id bf. Explanation: Write $f=\sum\hat{f}(\psi)\psi$. We have $$4^d(1-2\eps)\le \sum_{x,y}f(x+y)f(x)f(y)=\sum_{x,y,\psi_1,\psi_2,\psi_3}\hat{f}(\psi_1)\hat{f}(\psi_2)\hat{f}(\psi_3)\psi_1(x+y)\psi_2(x)\psi_3(y)=4^d\sum_\psi[\hat{f}(\psi)]^3 $$
by character orthogonality. Moreover $1=\inner{f,f}=\sum_\psi [\hat{f}(\psi)]^2$. Noting that the $\hat{f}(\psi)$ are real, we have $\max_\psi\hat{f}(\psi)\ge \dfrac{\sum_\psi [\hat{f}(\psi)]^3}{\sum_\psi [\hat{f}(\psi)]^2}\ge 1-2\eps$ and so for $\phi=\arg\max \hat{f}$ we have $f=\phi$ in probability $\ge 1-\eps$.   \\\\

id bl. Fact: Let $G$ be a finite abelian group. For a subgroup $H$ let $H^\perp=\set{\phi\in\widehat{G}:\phi\rvert_H\equiv 1}$. Then\\
\i  $H^\perp$ is a subgroup naturally isomorphic to $\widehat{G/H}$. \\
\ii $\widehat{G}/H^\perp$ is naturally isomorphic to $\widehat{H}$. \\
\iii $H^{\perp\perp}$ is naturally isomorphic to $H$.   \\
\iv We have $\displaystyle\sum_{\psi\in H^\perp}\psi(g) =\begin{cases}
			[G:H] & g\in H\\
            0 & g\not\in H
		 \end{cases}$ \\
\v {\it Poisson summation formula}: For $f:G\to\C$ we have $$\sum_{h\in H}f(h) = \abs{H} \sum_{\psi\in H^\perp}\hat{f}(\psi)$$
\\


id bl. Explanation: \\
\iv This follows from \i and character orthogonality.   \\
\v We may assume $f$ is a character of $G$. If $f\in H^\perp$ the formula is clear. If $f\nin H^\perp$ then $f\rvert_H$ is a non-trivial character on $H$ so that the lhs vanishes, as does the right.  \\\\


Nomenclature. A society has voters and politicians. A {\it voting system} is a mapping, taking in a linear preference of the politicians from each voter, and returning a linear preference of the politicians. The {\it majority} system works if there's an odd number of voters and two politicians, and ranks on top the politician the majority of voters ranked on top. The majority system is pretty fair, and on the other spectrum we have a {\it dictatorship}: a system which always returns the preferences of one specific voter, the dictator.    \\


id bp. Fact: Suppose a voting system on three politicians satisfies the following:  \\
\i If all the voters rank one politician higher than another, so will the system.  \\
\ii For any pair of politicians, the preference between the two returned by the system depends only on the preferences of the voters between the two.  \\
\iii If the voters choose their preferences (uniformly) at random, then any one of politician has even odds at ranking higher than any other.  \\
Then the system is a dictatorship.  \\\\


id bp. Explanation: We first need a model. If for example there's four voters, the three politicians are named $a,b,c$ and the preferences are
$a>b>c, \spc c>a>b, \spc b>a>c, \spc b>c>a$
then the preferences of the voters between any pair of pair of politicians are as follows:
$$\begin{bmatrix}
    a:b && a>b && a>b && b>a && b>a \\
b:c && b>c && c>b && b>c && b>c \\
c:a && a>c && c>a && a>c && c>a 
\end{bmatrix}$$
For each row, the voting system returns a preference of the two politicians. In other words, the system is composed of three functions taking in $n$ signs (where $n$ is the number of voters) and returning one sign. In our example, the preferences are described as the sign vectors
$$\begin{bmatrix}
x: & +1 & +1 & -1 & -1 \\
y: & +1 & -1 & +1 & +1 \\
z: & -1 & +1 & -1 & +1 
\end{bmatrix}  $$
and assumption \ii implies that the voting system returns a triple $(f(x),g(y),h(z))$ describing the final preferences $(a:b, b:c, c:a)$. It is important to see that each of $f,g,h$ is defined on the full space $\set{\pm1}^n$ but out of the $8^n$ triples $x,y,z$, only $6^n$ describe potential preferences of voters, namely for each $i\in[n]$ the three signs $x_i,y_i,z_i$ cannot all be the same. We draw at random preferences for the voters (out of a space of $6^n$ possibilities), described as sign vectors $x,y,z$. The three signs $f(x),g(y),h(z)$ are not all the same, meaning 
$$f(x)g(y)+g(y)h(z)+h(z)f(x)=-1$$
is a constant random variable. On the other hand
$$\E[f(x)g(y)]=\sum_{\psi_1,\psi_2\in\widehat{\set{\pm1}^n}}\hat{f}(\psi_1)\hat{g}(\psi_2)\E[\psi_1(x)\psi_2(y)]$$
Now a character $\psi$ corresponds to a set $S$ via $\psi(x)=\prod_{s\in S}x_s$. If $\psi_1\neq\psi_2$ then from $\E[\psi_1(x)\psi_2(y)]$ we may factor out a zero in the form of $\E x_j$ or $\E y_j$. Since $\E[x_jy_j]=-\frac{1}{3}$ we have  $\E[f(x)g(y)]=\sum_{S\sub[n]} \hat{f}(S)\hat{g}(S) \fit{-\frac{1}{3}}^{\abs{S}}$. In total we have
$$1 = \sum_{S\sub[n]}-\fit{-\frac{1}{3}}^{\abs{S}}\fit{\hat{f}\hat{g}+\hat{g}\hat{h}+\hat{h}\hat{f}}(S)$$
Now $\hat{f}(\empty)=\hat{g}(\empty)=\hat{h}(\empty)=0$ by assumption \iii Moreover $\abs{r_1r_2+r_2r_3+r_3r_1}\le r_1^2+r_2^2+r_3^2$ for real $r_i$ so 
$$1\le \sum_{\empty\neq S\sub[n]}\dfrac{\hat{f}(S)^2+\hat{g}(S)^2+\hat{h}(S)^2}{3^S}\le \dfrac{\inner{f,f}+\inner{g,g}+\inner{h,h}}{3}=1$$
implying the only non-trivial coefficients are singletons, so $f,g,h$ are linear functions. Since they only take two values, they're of the form $f(x)=\pm x_j$, $g(y)=\pm y_k$, $h(z)=\pm z_\ell$. Assumption \i yields positive signs, and it's easy to see $j=k=\ell$ is a dictator. \\\\


id bt. Fact: The set of homomorphisms $G\to\F^\times$ is a linear independent set in $G\to \F$. In particular, if $G$ is finite then $G$ admits at most $\abs{G}$ homomorphisms to $\F^\times$. \\\\


id bt. Explanation: Let $\phi_1,\dots,\phi_n$ be distinct homomorphisms and suppose $\sum_i \alpha_i\phi_i\equiv 0$. We have 
$$\sum_i \alpha_i\phi_i(h)\phi_i(g)=\sum_i \alpha_i\phi_i(hg)=0=\sum_i \alpha_i\phi_i(g)=\sum_i \alpha_i\phi_n(h)\phi_i(g)=$$
We may inductively assume $\phi_1,\dots,\phi_{n-1}$ are linearly independent. This yields $\alpha_i\phi_i(h)=\alpha_i\phi_n(h)$ $\forall i\in[n-1], h$ so $\alpha_1=\dots=\alpha_{n-1}=0\implies \alpha_n=0$.\\\\


id bz. Fact: The groups $\GL(3,2)$ and $\PSL(2,7)$ are isomorphic.  \\\\


id bz. Explanation: Both groups have order $168$. We shall find an isomorphism $\Psi:\PSL(2,7)\to\GL(3,2)$. As a $3$-dimensional $\F_2$ vector space we take $\F_8$, realized as $\F_2[t]/\gen{t^3+t+1}$ with $\alpha=\bar{t}$ being a generator for $\F_8^\times$. Thus ${\F_7}\cup\set{\infty}\longleftrightarrow\F_8=\set{\alpha^i}_{i=0}^6\cup\set{0}$ via $i\longleftrightarrow \alpha^i$, writing $\alpha^\infty=0$. Given a Möbius transformation $h$ of ${\F_7}\cup\set{\infty}$, set $T_h=\Psi(h):\F_8\to\F_8$, $\alpha^i\mapsto \alpha^{f(i)}+\alpha^{f(\infty)}$. Let us consider $3$ examples. If $h(z)=2z$ then $T_h(\gamma)=\gamma^2$. If $h(z)=z+1$ then $T_h(\gamma)=\alpha\gamma$. If $h(z)=-1/z$ then $T_h(b_0+b_1\alpha+b_2\alpha^2)=b_0+b_2\alpha+b_1\alpha^2$. The three Möbius transformations generate $\PSL(2,7)$, their images under $\Psi$ are additive functions of $\F_8$. We have that $\Psi$ is a homomorphism, since if  $f,g\in\PSL(2,7)$ are such that $\Psi_f,\Psi_g$ are additive, then by definition $\Psi_{f\circ g}=\Psi_f\circ\Psi_g$. Finally, $\Psi$ is a bijection, either because $\PSL(2,7)$ is simple and $\Psi$ is non-trivial, or because the above $3$ transformations generate $\GL(3,2)$. \\\\


id bz. Explanation 2: The group $\PSL(2,7)$ acts on size $4$ subsets of $\F_7\cup\set{\infty}$. The orbit of $\set{0,1,3,\infty}$ is:\\
$\set{0,1,3,\infty}, \set{2,4,5,6}$\\
$\set{1,2,4,\infty}, \set{3,5,6,0}$\\
$\set{2,3,5,\infty}, \set{4,6,0,1}$\\
$\set{3,4,6,\infty}, \set{5,0,1,2}$\\
$\set{4,5,0,\infty}, \set{6,1,2,3}$\\
$\set{5,6,1,\infty}, \set{0,2,3,4}$\\
$\set{6,0,2,\infty}, \set{1,3,4,5}$\\
These $14$ sets form a block design - every $3$ elements of $\F_7\cup\set{\infty}$ belong to precisely one set. In fact, the $14$ affine planes of $\F_2^3$ form an isomorphic design. For example we can match\\ 
$\infty\longleftrightarrow (0,0,0)$\\
$0\longleftrightarrow (1,0,0)$\\
$1\longleftrightarrow (0,1,0)$\\
$2\longleftrightarrow (0,0,1)$\\
$3\longleftrightarrow (1,1,0)$\\
$4\longleftrightarrow (0,1,1)$\\
$5\longleftrightarrow (1,1,1)$\\
$6\longleftrightarrow (1,0,1)$\\
Now, the group of automorphisms of the design is clearly the affine group $G=\set{v\mapsto Av+b:A\in\GL(3,2), b\in\F_2^3}$ and so we have a homomorphism $\PSL(2,7)\to G$. Composing with $G\to\GL(3,2)$ (via $Av+b\mapsto A$) we have $\PSL(2,7)\to \GL(3,2)$. But both groups have the same order $(168)$, the first is simple, and the homomorphism is non-trivial, so it is an isomorphism.\\\\


id cd. Fact: Any symmetric polynomial in $x_1,\dots,x_n$ can be expressed as a polynomial in the esp's $e_1,\dots,e_n$.\\\\


id cd. Explanation: We give a recursive algorithm. If $n=1$ then $e_1=x_1$ and we're done. If $P(x_1,\dots,x_n)$ is our symmetric polynomial, then $Q(x_1,\dots,x_{n-1})=P(x_1,\dots,x_{n-1},0)$ [namely the constant term of $P$ when expressed as a polynomial of $x_n$] is a symmetric polynomial of fewer variables, and we may express it as some polynomial $f$ of the esp's $e_1,\dots,e_{n-1}$ of $x_1,\dots,x_n$. A neat observation is that $e_i(x_1,\dots,x_{n-1})=e_i(x_1,\dots,x_{n-1},0)$, which suggests to look at $D(x_1,\dots,x_n)=P-f(e_1(x_1,\dots,x_n)\dots,e_{n-1}(x_1,\dots,x_n)$. Now $D$ is symmetric in all $n$ variables, and $D(x_1,\dots,x_{n-1},0)\equiv 0$. This means $x_1$ divides $D$, and so $x_1\dots x_n$ divides $D$. In other words $D=e_n R$ for some symmetric polynomial $R$ of smaller degree than $P$.






\newpage
%   %   %   %   %   %   %   %   %   %   %   %   %   %   %   %
% SECTION = CHAPTER 6 - ANALYSIS
\chap{6. Analysis}
Nomenclature. A function $f$ defined on the vertices of a graph will be called {\it{harmonic}} if for every vertex $v$ the average value of $f(u)$ over the neighbours $u$ of $v$ equals $f(v)$.  \\\\


id ae. Fact: Let $f$ be a real valued harmonic function on the lattice $\Z^d$. If $f$ is non-constant, then it is unbounded. \\\\


id ae. Explanation: If $f$ is non-constant, then wlog the bounded harmonic function $g(x)=f(x+e_1)-f(x)$ attains a positive value. Let $S=\sup g > 0$. Given $\eps$ we may find $x_\eps$ for which $g(x_\eps)>S-\eps$. Since the average of $g$ over the $2d$ neighbours of $x_\eps$ is larger than $S-\eps$, and each value is at most $S$, it follows that $g$ assigns each neighbour a value greater than $S-2d\eps$. Thus $g(x_\eps + e_1) > S-2d\eps$, and generally $g(x_\eps + ke_1) > S-(2d)^k\eps$. However, if we pick $k$ large enough and $\eps$ small enough, we can make $f(x+(k+1)e_1)-f(x)=g(x)+g(x+e_1)+\dots+g(x+ke_1) > (k+1)(S-(2d)^k\eps)$ arbitrarily large, meaning $f$ is unbounded.    \\\\


id af. Fact: Let $T$ be a strict contraction of a complete metric space $M$. Then in $M$ there exists a unique fixed point $m^*$ of $T$. Moreover, for any $m\in M$ we have $\displaystyle\lim_{n\to\infty} T^{ n}(m)=m^*$. \\\\


id af. Explanation: A strict contraction clearly cannot have more than one fixed point. Let $m$ be an arbitrary point, and let $m_n=T^n(m)$ be its orbit under $T$. We have $d(m_n,m_{n+1})\le q^{n}d(m,Tm)$, where $q$ is the contraction constant for $T$. Thus $\sum d(m_n,m_{n+1}) \le \frac{d(m,Tm)}{1-q}$ is convergent, implying $m_n$ is Cauchy, and hence convergent. By continuity, the limit of the orbit is a fixed point.  \\\\


id av. Fact: The Fourier transform is injective on $\LL^1(\R)$.     \\\\


id av. Explanation: Let $f\in\LL^1, \hat{f}\equiv 0$. For all $a,\xi\in\R$ we have
$$\int_{-\infty}^a f(t)e^{i\xi(t-a)}\d t=-\int_a^\infty f(t)e^{i\xi(t-a)}\d t$$
and we denote this value by $F_a(\xi)$. The right hand side may be analytically continued to arguments in the closed upper half plane, and the left hand side may be continued to the closed lower half plane. It follows that $F_a(\xi)$ is an entire function. However, it is everywhere bounded by $\norm{f}_1$, and so must be constant. By dominated convergence we have $\lim_{r\to\infty} F_a(ir)=0$, and so $F_a(\xi)=0$. In particular $\int_{-\infty}^a f(t)\d t=F_a(0)=0$ for all $a$ and so $f=0$ almost everywhere.    \\\\


id ag. Fact: Let $f\in\CC(\T)$ and let $F_m(t)=\displaystyle\sum_{j=-m}^m \hat{f}(j) e^{2\pi i jt}$ be the Fourier approximations of $f$. Then the sequence of averages $\sigma_n(t)=\dfrac{F_0(t)+\dots +F_n(t)}{n+1}$ converges uniformly to $f$. \\\\


id aq. Explanation: We have
$\hat{f}(j)e^{2\pi ijt}=\int_0^1 f(x)e^{2\pi i j(t-x)}\d x=\int_{-t}^{1-t}f(u+t)e^{-2\pi i j u}\d u=\int_{0}^{1}f(u+t)e^{-2\pi i j u}\d u$
and therefore 
$$F_m(t)=\int_0^1 f(u+t) P_m(u)\d u \phantom{---} \sigma_n(t)=\int_0^1 f(u+t) D_n(u)\d u $$
For
$$P_m(u)=\sum_{j=-m}^m e^{2\pi i j u} \phantom{---} D_n(u)=\dfrac{P_0(u)+\dots+P_n(u)}{n+1}$$
We have the identities $\displaystyle \int_0^1 D_n(u)=1$ and
$\displaystyle (n+1)D_n(u)=\fit{\sum_{k=0}^n e^{2\pi i(k-\frac{n}{2})u}}^2=\fit{\dfrac{\sin(\pi(n+1)u)}{\sin(\pi u)}}^2\ge0$. (To clarify, $D_n(u)=n+1$ for $u\in\Z$).
We continue
$$\sigma_n(t)-f(t)=\int_0^1  [f(u+t)-f(t)] D_n(u)\d u\implies \abs{\sigma_n(t)-f(t)}\le\int_0^1  \abs{f(u+t)-f(t)} D_n(u)\d u $$
Given $\eps$ we find $\delta$ such that
$\abs{x-y}\le\delta\implies\abs{f(x)-f(y)}\le\eps$.
The first part of the integral is bounded independent of $n$ or $t$: $ \int_{-\delta}^{\delta}  \abs{f(u+t)-f(t)} D_n(u)\d u\le \eps  \int_{0}^{1} D_n(u)\d u=\eps$. Finally, on $[\delta,1-\delta]$ we have $D_n(u)\le \dfrac{1}{(n+1)\sin(\pi\delta)^2}$
and so if $n$ is large enough then $D_n(u)\le \eps$ on this segment, and in total 
$\abs{\sigma_n(t)-f(t)}\le \eps+2\norm{f}_\infty\eps$
independent of $t$.     \\\\


id bc. Fact: Let $b,m,n$ be elements of a $C\str$ algebra with $m,n$ normal and $bn=mb$. Then $bn\str=m\str b$. \\\\


id bc. Explanation: We have $bn^k=m^kb$, and so $bf(n)=f(m)b$ for analytic functions $f$. In particular $b=\exp(\xi m)b\exp(-\xi n)$ for all $\xi\in\C$. Normality yields
$$\exp(i\zeta m\str)b\exp(-i\zeta n\str) = \exp(i(\zeta m\str + \zeta\str m))b\exp(-i(\zeta^* n-\zeta n\str))$$
The rhs is an entire function of $\zeta$, and the lhs is bounded by $\norm{b}$, and so the expression is a constant, namely $b$. Finally, the lhs coefficient of $\zeta$ is $im\str b-ibn\str$. \\\\


id bj. Fact: For a box $I\in\R^d$, not necessarily parallel to the standard axes, write $\rho(I)$ for the sum of the $d$ edge lengths of $I$. If $I_1\sub I_2$ then $\rho(I_1)\le \rho(I_2)$.   \\\\


id bj. Explanation: This is easy for $d=1,2$. For $d\ge 3$, we consider the volume of the set of points of distance $\le r$ from $I$, denoted $f(r;I)$, where we take $I$ as a solid. $f(r;I)=c_0 r^d + c_1\rho(I) r^{d-1}+\dots+\vol(I)$ is a polynomial in $I$ of degree $d$, and clearly $f(r;I_1)\le f(r;I_2)$ for all $r$.  \\\\


id bk. Fact: Let $I,I_1,I_2,\dots$ be intervals with $I\sub\bigcup_{k}I_k$. Then $\len(I)\le\sum_k \len(I_k)$. \\\\


id bk. Explanation: We first assume $I$ is compact and all the $I_k$ are open. Thus $I\sub I_1\cup\dots\cup I_N$ for some $N$, and we'll show $\len(I)\le\len(I_1)+\dots+\len(I_N)$ by induction on $N$. The base case is trivial. For the step, assume wlog $m=\min I\in I_1=(\alpha,\beta)$. Then $[\beta,\max I]\sub I_2\cup\dots\cup I_N\implies \len(I)< \len(I_1)+\len\fit{[\beta,\max I]}\le \len(I_1)+\dots+\len(I_N)$. A second step is to remove the openness assumption on the $I_k$, but keep the compactness assumption on $I$. Let $J_k$ be the interior of $I_k$, and for some fixed $\eps$ let $L_k,R_k$ be open intervals of length $\eps/2^k$ around the left and right endpoints of $I_k$ (if one of the $I_k$ is infinite there's nothing to prove). We get that $\len(I)\le \sum \len(I_k)+2\eps$, and the desired inequality follows in the limit. The final step of removing the compactness assumption on $I$ is by an approximation of $I$ using compact intervals.    \\\\

\newpage
\subsection*{Proving the central limit theorem}


We embark on a quest to prove the central limit theorem [bv]. Our best tool around independent random variables is interchanging expectation and products. The theorem has to do with sums rather than products, and this suggests raising an exponential.  \\\\ 


Nomenclature. The {\it characteristic function} of a random variable $X$ is $\vphi(\theta)= \E[e^{i\theta X}]=\E\cos(\theta X)+i\E\sin(\theta X)$. \\\\


id by. Examples:    \\
\i $X\sim U[-1,1] \implies \vphi(\theta)=\frac{\sin(\theta)}{\theta}$   \\
\ii $f(x)=\frac{1}{\pi(1+x^2)}\implies \vphi(\theta)=e^{-\abs{\theta}}$ \\
\iii $X\sim N(0,1)\implies \vphi(\theta)=e^{-\frac{1}{2}\theta^2}$  \\
\iv $X\sim\Exp(1)\implies \vphi(\theta)=\frac{1}{1-i\theta}$    \\
\v $f(x)=\frac{e^{-\abs{x}}}{2}\implies \vphi(\theta)=\frac{1}{1+\theta^2}$     \\
\vi $f(x)=(1-\abs{x})1_{\abs{x}\le1}\implies \vphi(\theta)=2\frac{1-\cos\left(\theta\right)}{\theta^{2}}$ \\
\vii $f(x)=\frac{1-\cos x}{\pi x^2}\implies \vphi(\theta)=(1-\abs{\theta})1_{\abs{\theta}\le 1}$
\\


id bx. Fact: The characteristic function $\vphi=\vphi_X$ is a well-defined continuous function $\R\to\C$, bounded in absolute value by $1$, and mapping $0$ to $1$. We have
$$\vphi_{aX+b}(\theta)=e^{i\theta b}\vphi_X(a\theta)$$ and $$\vphi_{X+Y}(\theta)=\vphi_X(\theta)\vphi_Y(\theta)$$
if $X,Y$ are independent.\\





\newpage
%   %   %   %   %   %   %   %   %   %   %   %   %   %   %   %
% SECTION = CHAPTER -1 - SOLUTIONS
\chap{-1. Solutions} \label{sol}
id aa. The probability is $1/101$, independent of $k=0,\dots,100$. To show this, note that we may generate a biased coin toss by generating a uniform random number $t$ from $[0,1]$ and then show heads iff $t<p$. Therefore the process can be described by generating $1+100$ independent uniform random numbers $p=t_0,t_1,\dots,t_{100}$ from $[0,1]$ and the number of heads is the index of $t_0$ after sorting, which is distributed uniformly in $0,\dots,100$.  \\\\


id ab. First consider the case $n=2$. The two players have numbers in $0,1$, and see the other person's number. To guarantee a correct guess, one will guess they have the same number, and the other will guess they have different numbers. We generalize this to arbitrary $n$ by assigning the players id's $0,1,\dots,n-1$, and the player with id $i$ guessing that the sum of all their numbers is congruent to $i$ modulo $n$. This strategy guarantees exactly one of them will be right, namely the one with id equal to the sum of their numbers modulo $n$.     \\\\


id ad. Let $e_n$ denote the $n$-th coordinate sequence, set $\alpha_n=f(e_n)$. We'll show that $\alpha_{N+1}=\alpha_{N+2}=\dots = 0$ after some value of $N$. To do this, we form a sequence $b=(b_1,b_2,b_3,\dots)$ as follows: $b_1=1$, and $b_{k+1}$ is a power of $2$ greater than both $b_k$ and $2\abs{f(b_1e_1+\dots+b_ke_k)}$. Note that $b_k\mid b_{k+1}$ and $\lim b=\infty$. Set $B=f(b)$. We have $B=f(b_1e_1+\dots+b_ke_k)+b_{k+1}f\fit{0..0,1,{b_{k+2}}/{b_{k+1}},{b_{k+3}}/{b_{k+1}},\dots}$. If $k$ is sufficiently large, we must have $B=f(b_1e_1+\dots+b_ke_k)$, or otherwise $|B|\ge b_{k+1} - \abs{f(b_1e_1+\dots+b_ke_k)}\ge b_{k+1}/2$. Therefore, if $k$ is sufficiently large, $\alpha_k=f(e_k)=0$. Now we may consider $g(x)=f(x)-\sum \alpha_i x_i$ as an additive function satisfying $g(e_n)=0$ for all $n$, and we wish to show $g(x)\equiv 0$. Write $x_n = 2^n d_n + 3^n r_n$ for some integer sequences $d,r$. We have $g(x)=g(2d_1,4d_2,8d_3,\dots)+g(3r_1,9r_2,27r_3,\dots)$, but since $g(e_n)=0$, the left summand is divisible by all powers of two, and the right summand is divisible by all powers of three, so they are both zero, and $g(x)\equiv0$. \\\\


id ah. Subtracting a linear factor, we may assume $f(0)=f(1)=0$, with the intention of showing $f\equiv 0$. Let $f_\eps(t)=f(t)-\eps t(1-t)$. Then $\displaystyle\lim_{h\to 0}\dfrac{f_\eps(t_0+h)-2f_\eps(t_0)+f_\eps(t_0-h)}{h^2}=\eps$, implying $f_\eps$ does not attain its maximum at any inner point $t_0\in(0,1)$. Therefore $f_\eps\le 0$, and in the limit $f\le 0$. Applying the same to $-f$ we have $f\equiv 0$.   \\\\


id ai. What demands explanation is that all smooth/continuous homomorphisms are exponentials. Indeed, for $\phi$ smooth $$\phi(s+t)=\phi(s)\phi(t)\implies \phi'(s+t)=\phi'(s)\phi(t)\implies \phi'(t)=\phi'(0)\phi(t)\implies\phi(t)=\exp(t\phi'(0))$$
It remains to show that a continuous homomorphism is smooth. Indeed, we have $\int_x^{x+a}\phi(t)\d t=\phi(x)\int_0^a\phi(t)\d t$. We pick a small enough $a$ so that $\frac{1}{a}\int_0^a\phi(t)\d t=\id + o(1)$ is invertible, yielding
$$\phi(x)=\fit{\int_x^{x+a}\phi(t)\dt}\fit{\int_0^a\phi(t)\dt}\inv$$
is smooth.  \\\\


id aj. Suppose $\sum a_n^2$ diverges, and form a partition $0=N_0<N_1<\dots$ for which $s_k=\sum_{(N_{k-1},N_k]}a_n^2 > 1$. The sequence $b_n=a_n/ks_k$ (where $n\in (N_{k-1},N_k]$) is in $\ell^2$, since $\sum b_n^2=\sum_k \frac{1}{k^2s_k}$. However, $\sum a_nb_n=\sum_k 1/k$ diverges.    \\\\


id aw. The number $100$ can only be the top card once, after that it will stay forever at the bottom. Between any two times the number $99$ is on top, $100$ has to be on top. Between any two times the number $98$ is on top, either $100$ or $99$ has to be on top, ..., between any two times the number $2$ is on top a number bigger than $2$ has to be on top. Therefore each number except for $1$ can only be on top finitely many times.  \\

id aw. Solution 2. If $100$ ever becomes the top card, it'll later be stuck at the bottom. If not, the bottom card will never change, and the game will be indifferent to switching the numbers of the bottom card with $100$. So in either case we have an induction step. \\

id aw. Solution 3. Let the score of a deck be the sum of $2^i$ for the cards $i$ placed correctly at $i$. If the top card $k$ is not one, the score grows: a $2^k$ is added and at worst $1+2+\dots+2^{k-1}$ is subtracted. Since the score can't grow forever, eventually the top card becomes $1$.    \\

id aw. Solution 4. Eventually the state of the deck enters a loop. Once we're in the loop, the maximal number appearing on top has to be $1$, otherwise the remaining numbers on top would keep it away from comming back on top.   \\\\

id bh. Explanation: \\
\i we have $\E[f(S_n)]=\int_{x_i\ge0}f(\sum x_i)\lambda^n e^{-\lambda\sum x_i}\d x_1...\d x_n$. Change variables to $y_i=x_i$ for $i<n$ and $t=\sum x_i$. The Jacobian is $1$, the domain $0\le y_i, \sum y_i\le t$, and we get $\lambda^n\int_{t=0}^{\infty}\d t f(t)e^{-\lambda t} \int_{y_i\ge 0, \sum y_i\le t}\d y_1...\d y_{n-1} $. We recognise the inner volume as $\frac{t^{n-1}}{(n-1)!}$. Finally, $\frac{\d^{n-1} \LL[f](\lambda)}{\d \lambda^{n-1}}=\int_0^{\infty} (-x)^{n-1} e^{-\lambda x}f(x)\d x$.  \\
\ii We have $\E[X]=\lambda\inv$, $\var X=\lambda^{-2}$, so for $\lambda=n/y$ we have $\E[S_n]=y$, $\var(S_n)=y^2/n$. Chebyshev gives $\P[\abs{S_n-y}>yn^{-1/4}]\le n^{-1/2}$. If $\eps_n=\displaystyle\max\abs{f(\alpha)-f(\beta)}$ over $\set{\abs{\alpha-\beta}\le yn^{-1/4}, 0\le\alpha,\beta\le 2y}$, then $\eps_n$ tends to $0$ by uniform continuity, and $\abs{f(y)-\E[f(S_n)]}\le\E[\abs{f(y)-f(S_n)}]\le 2\norm{f}_\infty n^{-1/2} + \eps_n$ tends to $0$, so $f(y)=\lim\E[f(S_n)]$.   \\\\


id br. Let $Z_k$ be our capital after the $k$-th bet. We have $\E[Z_{k+1}|Z_k]\le 2pZ_k$, with equality iff we bet all our money on the $k+1$-st round. Thus the strategy maximizing $\E[Z_n]$ is to always bet everything, and has a value of $\$(2p)^n$. This strategy is extremely risky, since only in probability $p^n$ we don't lose our initial investment. Replacing the utility $Z$ by $\log(Z)$ has two effects: it greatly punishes losing all your capital, and returns are diminishing. Now if we wager a proportion $\alpha$ of our capital $Z_k$ in the $k+1$-st round we'll have $\E[\log(Z_{k+1})| Z_k,\alpha]=p\log(Z_k(1+\alpha)) + q\log(Z_k(1-\alpha))$ where $q=1-p$. This is maximized for $\alpha=2p-1$. Thus the optimal strategy always wager a proportion of $2p-1$ of our capital (which could be guessed, since for $p=1/2$ we'd not wager anything and for $p=1$ we'd be all in). This strategy has $\E[\log(Z_n)]=n(p\log p +q\log q+\log 2)$.   \\

id br. engineer's solution: Suppose we always wager a fixed proportion $\alpha$ of our capital. We'll have approximately $pn$ wins and $qn$ losses, meaning $Z_n\approx (1+\alpha)^{pn}(1-\alpha)^{qn}$. Thus $\log(Z_n)\approx n(p\log(1+\alpha)+q\log(1-\alpha))$ is to be maximized, as before.  \\\\


id bu. Let $q_n$ be an enumeration of $\Q$. Let $s_0(n)=n$ and $s_{k}(n)$ a sub-sequence of $s_{k-1}(n)$ for which $F_{s_k(n)}(q_k)$ converges. Let $s(n)=s_n(n)$. Then $H(q)=\lim_n F_{s(n)}(q)$ exists for all $q\in\Q$. We take $F(x)=\displaystyle\inf_{q>x}H(q)$. Note that $H,F$ are weakly-increasing and take values in $[0,1]$. Let us show that $F$ is right-continuous: If $x,\eps$ are given we may find $x<q$ so that $H(q)\le F(x)+\eps$. It follows that for any $x\le y<q$ we have $F(x)\le F(y)\le F(x)+\eps$. For any $x$ we have 
$$\sup_{q<x}H(q)=\sup_{q<x}\lim_n F_{s(n)}(q)\le \liminf_n F_{s(n)}(x)\le \limsup_n F_{s(n)}(x)\le\inf_{x<q'}\lim_n F_{s(n)}(q')=\inf_{x<q'}H(q')=F(x)$$
Now, if $x$ is a continuity point of $F$, we fix rational sequences $x-\frac{1}{n}< r^1_n<r^2_n<x$ so that $F(x)\leftarrow F(r^1_n)\le H(r^2_n)\le \sup_{q<x} H(q)$ which yields $\lim_n F_{s(n)}(x)=F(x)$. The last part is immediate. \\\\

id cf. The numbers $5$ and $504$ are related by [ce] where $L(\tau)=\dfrac{1}{504}\fit{1-\dfrac{27g_3(\tau)}{8\pi^6}}=\sum_{\ell}\sigma_5(\ell)q^\ell=\sum_{k,n}k^5q^{kn}=\sum_k\dfrac{k^5 q^k}{1-q^k}=R(q)$. We are almost there, except we want $1+q^k$ in our denominator, and only odd $k$. Here we need the following: $\dfrac{q^k}{1+q^k}=\dfrac{q^k}{1-q^k}-2\dfrac{q^{2k}}{1-q^{2k}}$. Hence $F(q)=\displaystyle\sum_{2\nmid k}\dfrac{k^5q^k}{1+q^k}=R(q)-32R(q^2) - 2\fit{R(q^2)-32R(q^4)}=R(q)-34R(q^2)+64R(q^4)$. However, $G_6(i)=0$ and $G_6(i/2)=-64G_g(2i)$ and so the $g_3$ parts cancel and we are left with $F(\tau=i/2, q=e^{-\pi})=31/504$.\\\\


id cq. We give an algorithm for writing $A=\pmat{a}{b}{c}{d}\in\SL_2(\Z)$ using $T,S$. We have $T^k A=\pmat{a+kc}{b+kd}{c}{d}$ and $SA=\pmat{-c}{-d}{a}{b}$. As long as $c\neq 0$, we write $a=nc+r$ for some $n\in\Z$, $0\le r<\abs{c}$. Then $A'=ST^{-n}A$ has $\abs{c'}<\abs{c}$. We keep these left multiplications of $A$ until $c=0$. But if $c=0$ then $A=\pm T^m$ for some $m\in\Z$, while $-\id =S^2$.\\\\


id ct. \i is trivial. For \ii, note that if $\eps\in[0,\frac{1}{2}]$ then we indeed have a $9$-periodic $$1,\eps,\eps-1,1-2\eps,2-3\eps,1-\eps,-1+2\eps,-\eps,1-\eps,1,\eps,\dots$$
we can positively scale any sequence, and the consecutive ratios precisely cover the entire $\R\cup\infty$ while also accounting for possible signs.\\\\


id cu. \i We must sacrifice the first. He codes the pairity of the black hats among the rest.\\\\
\ii Using the axiom of choice we fix for each class of binary sequences differing finitely from each other one representative sequence. Each player knows their sequence of colors up to a finite difference, so knows the class and its representative. The first player codes whether there's an even or odd discrepency with the representative sequence.\\\\
A different solution for the infinite case - using Zorn's lemma we fix a linear function $f:\F_2^\N\to\F_2$ with $f(e_n)=1$ for all $n$. Then the first player declares the value of $f(x)$ where $x$ is the sequence of hats he sees.\\\\


id cv. We translate $\Q$ to $\N$. Then it's easy - each guesses they have the smaller number and guesses all numbers below that of the other.\\\\
We may pick the smallest uncountable ordinal to replace the rationals. They'll use the same strategy as above.\\\\
In case there's three of them, again each guesses they're the smallest. They look at the other two (elements of the smallest uncountable ordinal) and fix the larger, $\alpha$. [Assuming they are not the maximal, both their hat and the other hat they see are elements of $\alpha$] They match $\alpha$ with $\N$ (in a pre-determined way using the axiom of choice) and list all the elements of $\alpha$ less than that of the other hat they see.\\\\


id cw. Using the axiom of choice each class of binary sequences differing finitely from each other is fixed with a representative sequence. They divide $a=(a_n)$ to $100$ sequences $b_i=a_{i},a_{i+100},a_{i+200}+\dots$. The $i$-th player is {\it in charge} of $b_i$. He inspects all the other $b_j$. For each of them, he looks at the representative for $b_j$, say $c_j$ and writes down the first index from which they agree, $I_j$. Then he lets his uninspected index be the $M^{i}=\max_{j\neq i}I_j$ of the sequence he is in charge of, $b_i$. He inspects all other elements in $b_i$ and guesses $c_i[M^{i}]$. The only one that gets it wrong (maybe) is the one with the maximal $I_j$.\\\\


id cx. We correspond the squares with $0,\dots,63$ in some natural way, and let $\oplus$ denote nim-sum/xor/binary sum without carrying of numbers in that range. The second magician computes the nim-sum of all squares showing tails. The first magician sees the initial board and computes the nim-sum of all the tails there, $a$. If $b$ is the picked square, he lets $c=a\oplus b$ and flips the coin at $c$. This causses the nim-sum to go from $a$ to $a\oplus c=b$, since adding $c$ or subtracting $c$ from a nim-sum is the same thing ($c\oplus c=0$).\\\\
In the infinite version, we keep the nim-sum on $\N$. The problem is we can't take infinite nim-sums. Here is how we can solve this - using the axiom of choice, we fix a linear function $f:\F_2^\N \to (\N,\oplus)$ such that $f(e_i)=i$. The second magician guesses $f(x)$ where $x$ is what they see, the first magician flips $f(a)+b$ where $a$ is the board they get from the audience and $b$ is the picked square.\\\\


id cy. We fix a well-order on $2^M$. Let $f:M\to2$ be the coloring of the mathematicians. Each mathematician $m$ constructs $F_m=\set{g\in M\to 2: g(m')=f(m') \spc\forall m'>m}$ and $g_m=\min F_m$, and guesses his color is $g_m(m)$. If there's infinitely many wrong guessers, we can take a chain $m_1<m_2<\dots$. But now $g_{m_{i+1}}\le g_{m_i}$ since $F_{m_i}\sub F_{m_{i+1}}$. The sequence of $g$'s can't decrease forever, it must stabilize. But once $g_{m_{N+1}}=g_{m_N}$ then $m_{N+1}$ makes the correct guess.\\\\


id cz. Ultrafilters.\\\\


id da. Let $A$ be a set of representative for $(0,\infty)/\Q$. Then $\abs{A}=2^{\aleph_0}$. We define $f(x)=a+\dfrac{1}{n+1}$ if $x=a+\dfrac{1}{n}$ for some $a\in A$, $n\ge 1$, and otherwise $f(x)=0$. The set of limits is $A\cup\set{0}$.\\\\


id db. Let $A$ be a Hamel basis for $R/\Q$. For each $x\in \R$ write it as $x=q_1\alpha_1+\dots+q_k\alpha_k$ for some $k\ge 0$, $q_i\in\Q$, $\alpha_i\in A$ with $\alpha_1<\dots<\alpha_k$. Set $f(x)=(q_1,\dots,q_k)\in \bigcup_d \Q^d$ (a countable set). Arguing inductively over the minimal basis element appearing in either $x,x+2r$ [with non-zero coefficient] we see that if the AP is not separated by $f$ then $x=x+2r$.\\\\


id dc. Each prisoner is assigned a number in $1,\dots,31$. They assign the total state (the hat color assignment) with a number in $0,\dots,31$ by xor-ing the numbers of the prisoners with a black hat. If this value is $a$, then player $i$ knows it's either $a$ or $a\oplus i$. Each does the following: if both possibilities for the values are non-zero, pass. If one is zero, guess the hat color which would make it not. In case $a=0$ (of probability $1/32$) they all guess wrong and lose. Otherwise, only one player takes a guess (namely player $a$) and is correct.\\\\
In the infinite case, we can use the same solution as the infinite case of [cx] to make "infinite xoring" make sense. Again the only time they lose is when $f$ of the state is $0$ and they all guess wrong.\\\\


id dd. Define a (non-associative) multiplication $\star$ on the set of colors: if two colors are equal their $\star$ equals them, otherwise their $\star$ is the third color. The first person says $(((c_1\star c_2)\star c_3)\dots)\star c_n$.\\\\
A different solution: if $3\nmid n$ (the number of prisoners except the first) then counting hat colors modulo $3$ we get two equal residues and an odd-one-out. The first person says the odd-one-out. If $3\mid n$ we pretend there's an extra person in front of the last with the same color as the last and employ the same trick.\\\\ 


id de. For $a\le b$ in $[0,2\pi]$ we have $$2\abs{f(b)-f(a)}=\abs{\int_a^b f'(t)\d t -\int_b^{a+2\pi}f'(t)\d t}\le\int_{a}^{a+2\pi}\abs{f'(t)}\d t = \norm{f'}_1$$
Thus for all $T$ we have $$\abs{f(T)} = \abs{\E_t [f(T)-f(t)]}\le \E_t\abs{f(T)-f(t)}\le \frac{1}{2}\norm{f'}_1 $$.\\\\


id df. Construct a binary word $\omega$ of length $n-1$ where $\omega_i=1$ if $i$ is in the first $i$ places. If a $1$ in $\omega$ turns to a $0$, it means a more significant digit turned from $0$ to $1$. So $\omega$ is weakly increasing. Similarly $\omega'$ is increasing with $\omega'_i=1$ if $n+1-i$ is in the last $i$ places. But no matter what, either $\omega$ or $\omega'$ must change every step. So $\omega+\omega'$ is strongly increasing each step. \\\\

\newpage
%   %   %   %   %   %   %   %   %   %   %   %   %   %   %   %
% SECTION = CHAPTER -2 - NOTES
\chap{-2. Notes}
The following is a minimal attempt at giving due credit.    \\\\
id av. The given explanation is attributed to D.J. Newman.    \\\\


id ax. The given explanation is attributed to M. Nair.      \\\\


id be. The given explanation for $\F=\Q$ is attributed to S. Vishwanathan.  \\\\


id bg. The given explanation is attributed to S. Vishwanathan.  \\\\


id bm. The Fact is attributed to F.J. MacWilliams.  \\\\


id bv. The fact is known as the {\it central limit theorem}.    \\\\


id bw. The fact is attributed to Lévy.  \\\\


id bz. The explanation is due to E. Brown and N. Loehr.\\\\


id cn. The given explanation for \ii is due to Z.Y Kong and L.P Teo.\\\\


id df. The given solution is due to L. Radzivilovsky and G. Yurgin.\\\\

\end{document}
